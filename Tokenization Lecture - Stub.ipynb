{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the lectures on tokenization, normalization, and stemming. \n",
    "\n",
    "Before we get started, you're going to need the NLTK book corpus. Here are the steps to install it: \n",
    "\n",
    "1. Open a console or command window.\n",
    "1. Type `python` to start using python. \n",
    "1. Type `import nltk` and hit enter.\n",
    "1. Type `nltk.download()` and hit enter.\n",
    "1. This will open a little window. \n",
    "1. Click \"All Packages\" at the top of the list. \n",
    "1. Click \"Download\"\n",
    "\n",
    "Let me know if you run into any issues!\n",
    "\n",
    "---\n",
    "\n",
    "Now we can get started in earnest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by which we split text up into tokens. The simplest tokens are those split by whitespace. Let's begin by counting the words in a file that I've included the in repo: the text of _Beowulf_. \n",
    "\n",
    "1. Read the file into a variable that holds a (large) string. \n",
    "1. Look at the first 1000 characters of that string.\n",
    "1. Split that string on whitespace (spaces, returns, tabs, etc.)\n",
    "1. Count the number of tokens. \n",
    "1. Determine the most common 10 tokens. \n",
    "1. Find the tokens that include punctuation within the token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in beowulf here\n",
    "beowulf = open(\"beowulf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BEOWULF\\n\\nBy Anonymous\\n\\nTranslated by Gummere\\n\\n\\n\\n\\nBEOWULF\\n\\n\\n\\n\\nPRELUDE OF THE FOUNDER OF THE DANISH HOUSE\\n\\n\\n\\nLO, praise of the prowess of people-kings\\nof spear-armed Danes, in days long sped,\\nwe have heard, and what honor the athelings won!\\nOft Scyld the Scefing from squadroned foes,\\nfrom many a tribe, the mead-bench tore,\\nawing the earls. Since erst he lay\\nfriendless, a foundling, fate repaid him:\\nfor he waxed under welkin, in wealth he throve,\\ntill before him the folk, both far and near,\\nwho house by the whale-path, heard his mandate,\\ngave him gifts:  a good king he!\\nTo him an heir was afterward born,\\na son in his halls, whom heaven sent\\nto favor the folk, feeling their woe\\nthat erst they had lacked an earl for leader\\nso long a while; the Lord endowed him,\\nthe Wielder of Wonder, with world's renown.\\nFamed was this Beowulf:  {0a} far flew the boast of him,\\nson of Scyld, in the Scandian lands.\\nSo becomes it a youth to quit him well\\nwith his father's friends, by fee and gift,\\nthat to aid \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 1000 characters\n",
    "beowulf[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it on whitespace\n",
    "beo_tokens = beowulf.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of tokens\n",
    "len(beo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1701),\n",
       " ('of', 1032),\n",
       " ('and', 689),\n",
       " ('to', 531),\n",
       " ('in', 452),\n",
       " ('his', 428),\n",
       " ('that', 322),\n",
       " ('he', 312),\n",
       " ('with', 286),\n",
       " ('was', 240)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the 10 most common tokens\n",
    "# Hint: check out the Counter object in the collections library\n",
    "\n",
    "beo_counter = Counter(beo_tokens)\n",
    "beo_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LO,', 'people-kings', 'spear-armed', 'Danes,', 'sped,', 'heard,', 'won!', 'foes,', 'tribe,', 'mead-bench', 'tore,', 'earls.', 'friendless,', 'foundling,', 'him:', 'welkin,', 'throve,', 'folk,', 'near,', 'whale-path,', 'mandate,', 'gifts:', 'he!', 'born,', 'halls,', 'folk,', 'while;', 'him,', 'Wonder,', \"world's\", 'renown.', 'Beowulf:', '{0a}', 'him,', 'Scyld,', 'lands.', \"father's\", 'friends,', 'gift,', 'him,', 'aged,', 'days,', 'willing,', 'nigh,', 'loyal:', 'clan.', 'moment,', 'God.', \"ocean's\", 'billow,', 'clansmen,', 'them,', 'Scyld,', 'ruled....', 'ring-dight', 'vessel,', 'ice-flecked,', 'outbound,', \"atheling's\", 'barge:', 'boat,', 'breaker-of-rings,', '{0b}', 'one.', 'him.', 'battle,', 'blade:', \"o'er\", 'away.', 'gifts,', \"thanes'\", 'treasure,', 'seas,', 'child.', \"o'er\", 'standard,', 'gold-wove', 'banner;', 'him,', 'ocean.', 'spirits,', 'mood.', 'sooth,', 'halls,', \"'neath\", 'heaven,', '--', 'freight!', 'Scyldings,', 'beloved,', 'folk,', 'world,', 'heir,', 'Healfdene,', 'life,', 'sturdy,', 'glad.', 'Then,', 'one,', 'him,']\n",
      "5921\n"
     ]
    }
   ],
   "source": [
    "# Now calculate how many tokens have punctuation in them. \n",
    "# Hint: the string library has an object with all the punctuation\n",
    "# marks in it\n",
    "from string import punctuation\n",
    "\n",
    "punc_set = set(punctuation)\n",
    "\n",
    "beo_tokens_punct = []\n",
    "\n",
    "for w in beo_tokens :\n",
    "    w_set = set(w)\n",
    "    overlap = w_set.intersection(punc_set)\n",
    "\n",
    "    if len(overlap) > 0 :\n",
    "        beo_tokens_punct.append(w)\n",
    "\n",
    "print(beo_tokens_punct[:100])\n",
    "print(len(beo_tokens_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you look at those tokens with punctuation, what do you notice? \n",
    "\n",
    "What fraction of tokens contain punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2267192525654771"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beo_tokens_punct)/len(beo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Second Exercise\n",
    "\n",
    "Now let's try working with some NLTK data. Count the words (or, more precisely, tokens) in one of the first three books included in the book corpus (_Moby Dick_, _Sense and Sensibility_, and The Book of Genesis from the _Bible_).\n",
    "\n",
    "1. Pick one of the texts and assign it to a new variable. It'll have a name like `text1` before you assign it. That variable was created when we imported everything from `nltk.book`.\n",
    "1. Look at the structure of variable.\n",
    "1. Count the tokens as above. Use the `Counter` object.\n",
    "1. Display the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Assign the text to the new variable.\n",
    "\n",
    "texts()\n",
    "\n",
    "monty = text6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16967"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the tokens\n",
    "len(monty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 1197),\n",
       " ('.', 816),\n",
       " ('!', 801),\n",
       " (',', 731),\n",
       " (\"'\", 421),\n",
       " ('[', 319),\n",
       " (']', 312),\n",
       " ('the', 299),\n",
       " ('I', 255),\n",
       " ('ARTHUR', 225)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the 10 most common tokens.\n",
    "\n",
    "monty_counter = Counter(monty)\n",
    "monty_counter.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
