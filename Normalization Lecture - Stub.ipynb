{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "Normalizing text is a critical component of text mining and a step we'll take on every single analysis. Eventually it'll get to the point that it's basically second nature. This notebook accompanies the lecture, where we mention six common types of text normalization: \n",
    "\n",
    "1. Case folding\n",
    "1. Removing punctuation\n",
    "1. Handling numbers, dates, and times\n",
    "1. Extracting special information\n",
    "1. Removing stopwords\n",
    "1. Correcting spelling\n",
    "\n",
    "We'll work through a few examples of most of these, although we'll save spelling correction for another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Case Folding\n",
    "\n",
    "We'll often discover that having a mixture of upper and lower case doesn't serve us very well. Case folding helps us handle this. Let's start by finding all the words that appear in the top 1000 most frequent words in the chat corpus with multiple capitalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "texts()\n",
    "chat = text5\n",
    "chat_count = Counter(chat)\n",
    "\n",
    "collisions = dict()\n",
    "\n",
    "for word, count in chat_count.most_common(1000) :\n",
    "    lc_word = word.lower\n",
    "\n",
    "if lc_word not in collisions :\n",
    "    collisions[lc_word] = [word]\n",
    "\n",
    "else :\n",
    "    collisions[lc_word].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, wlist in collisions.items():\n",
    "    if len(wlist) > 1 :\n",
    "        print(f'The words {\",\".join(wlist)} map onto {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a slightly easier one, how many times are \"the\" and \"The\" used in _Moby Dick_? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13721"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "texts()\n",
    "Counter(text1)['the' or 'The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Punctuation\n",
    "\n",
    "Punctuation can be tricky to handle. The easiest thing is to remove it, but that's not always the best thing to do. To practice playing around with it, count the number of **unique** words that have punctuation in them _Beowulf_. Print out a few to look at (although there are a lot, so maybe don't print them all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n",
      "['wiles,', \"God's\", 'burgstead!', 'chose.', 'other.', 'Gold-gay', 'well-known', 'gold-crown', 'gone.', '{35a}', 'kin;', 'Gifths,', 'taunt,', 'threatened.', 'battle,', 'precious,', 'watch,', 'face-cloth.', 'freight!', 'man.']\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "punct_set = set(punctuation)\n",
    "punct_words = set() # since we want uniques\n",
    "\n",
    "for word in beowulf.split() :\n",
    "    wset = set(word)\n",
    "    if punct_set.intersection(wset) :\n",
    "        punct_words.add(word)\n",
    "    \n",
    "print(len(punct_words))\n",
    "\n",
    "# Let's print 20 or so\n",
    "print(list(punct_words)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the number of words that have punctuation in the _middle_ of the word. Let's also throw them in a `Counter` object and look at the most common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "punct_set_2 = set() \n",
    "\n",
    "for word in beowulf.split() :\n",
    "    if not word.isalnum() :\n",
    "        punct_set_2.add(word)\n",
    "\n",
    "print(len(punct_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "There are many common words that don't help analysis that much (and can take up a lot of space). These are called stopwords. Let's play around with the English stopwords.\n",
    "1. Load in the English stopwords and assign them to a variable called `sw`. Print them out. Any surprises?\n",
    "1. Look at the top words in _Moby Dick_ and _Sense and Sensibility_.\n",
    "1. Look at the top words in both of those that _aren't_ stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9397),\n",
       " ('to', 4063),\n",
       " ('.', 3975),\n",
       " ('the', 3861),\n",
       " ('of', 3565),\n",
       " ('and', 3350),\n",
       " ('her', 2436),\n",
       " ('a', 2043),\n",
       " ('I', 2004),\n",
       " ('in', 1904)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9397),\n",
       " ('to', 4063),\n",
       " ('.', 3975),\n",
       " ('the', 3861),\n",
       " ('of', 3565),\n",
       " ('and', 3350),\n",
       " ('her', 2436),\n",
       " ('a', 2043),\n",
       " ('I', 2004),\n",
       " ('in', 1904)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text2).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 906),\n",
       " ('one', 889),\n",
       " ('like', 624),\n",
       " ('upon', 538),\n",
       " ('man', 508),\n",
       " ('ship', 507),\n",
       " ('Ahab', 501),\n",
       " ('ye', 460),\n",
       " ('old', 436),\n",
       " ('sea', 433)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([w for w in text1 if w.lower() not in sw and w.isalpha()]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process by which we move from a token to some \"root\" of that word. Let's explore one of the stemmers available through NLTK.\n",
    "\n",
    "First, let's find all the words in the NLTK words corpus that end in \"ing\", then let's find those that have no vowels before an instance of \"ing\". You can access the words corpus with the confusing call of `nltk.corpus.words.words()`. To make it easier to deal with \"y\", let's just consider it a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "words = nltk.corpus.words.words()\n",
    "vowels = set('aeiouy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5557"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_words = [w for w in words if len(w) > 3 and w[-3:]==\"ing\"]\n",
    "len(ing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with the Porter Stemmer in NLTK. First we'll look at a few hundred characters of inaugural addresses both stemmed and not stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its leading principle the duty of shaping their measures so as to produce the greatest good to the greatest number . But with these broad admissions , if we would compare the sovereignty acknowledged to exist in the mass of our people with the power claimed by other sovereignties , even by those which have been considered most purely democratic , we shall find a most essential difference . All others lay claim to power limited only by their own will . The majority of our citizens , on the contrary , possess a sovereignty with an amount of power precisely\n",
      "\n",
      "\n",
      "\n",
      "aid of that almighti power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . the broad foundat upon which our constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . if such is it theori , those who are call upon to administ it must recogn as it lead principl the duti of shape their measur so as to produc the greatest good to the greatest number . but with these broad admiss , if we would compar the sovereignti acknowledg to exist in the mass of our peopl with the power claim by other sovereignti , even by those which have been consid most pure democrat , we shall find a most essenti differ . all other lay claim to power limit onli by their own will . the major of our citizen , on the contrari , possess a sovereignti with an amount of power precis\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 200\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for you: how many words are in the inaugural addresses? How many lowercase stems are in them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10025\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5631\n"
     ]
    }
   ],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Okay, let's have some \"fun\" and play around with some sets of characters that aren't words. Text 5 is the chat corpus. Find the emojis in there (doesn't have to be perfect) and count up the happy and sad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':P', ':):):)', ':-)', '9:10', ';)', '2:55', \"='s\", '3:45', '6:38', ';p', '.;)', '=]', ']:)', '=(', ':/', ';', '=/', ':', '=)', ':-o', ':-(', '=p', ':blush:', ';0', '=O', ':(', ';-)', '.:', '=-\\\\', 'n;t', '10:49', '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR', ':-@', '6:53', ':o *', '6:41', 'http://www.shadowbots.com', '; ..', '4:03', '=', ';]', 'd=', '6:51', '7:45', '>:->', ':|', ':love:', '=D', ':tongue:', ';-(', '!=', ':]', 'o<|=D', 'capab;e', ':D', ':@', ':O', '=[', ':)', ':p', ':beer:', ':.', 'http://forums.talkcity.com/tc-adults/start '}\n"
     ]
    }
   ],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}\n",
    "\n",
    "print(potential_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
